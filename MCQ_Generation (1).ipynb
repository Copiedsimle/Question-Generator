{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aweZgxXBDsOQ",
        "outputId": "cdd10ecc-8dd1-4dbb-e49a-20851d1fbd17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers\n",
        "!pip install sentencepiece\n",
        "!python -m nltk.downloader punkt\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk3X75xXRH5r"
      },
      "outputs": [],
      "source": [
        "text1 = \"Gravity (from Latin gravitas, meaning 'weight'), or gravitation, is a natural phenomenon by which all \\\n",
        "things with mass or energy—including planets, stars, galaxies, and even light—are brought toward (or gravitate toward) \\\n",
        "one another. On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides. \\\n",
        "The gravitational attraction of the original gaseous matter present in the Universe caused it to begin coalescing \\\n",
        "and forming stars and caused the stars to group together into galaxies, so gravity is responsible for many of \\\n",
        "the large-scale structures in the Universe. Gravity has an infinite range, although its effects become increasingly \\\n",
        "weaker as objects get further away\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvBFbCVGELuW"
      },
      "source": [
        "## Single task QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axwMJGNAPtHc",
        "outputId": "8657aedf-1891-44e4-876b-b077b147449e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Sample dataset\n",
        "paragraphs = [\n",
        "  \"The sky is blue. The grass is green.\",\n",
        "  \"The quick brown fox jumps over the lazy dog.\",\n",
        "  \"Roses are red. Violets are blue.\"\n",
        "]\n",
        "\n",
        "# Function to convert a paragraph to a list of sentences\n",
        "def paragraph_to_sentences(paragraph):\n",
        "    return paragraph.split('. ')\n",
        "\n",
        "# Convert the paragraphs to a list of sentences\n",
        "sentences = []\n",
        "for paragraph in paragraphs:\n",
        "\n",
        "    sentences.extend(paragraph_to_sentences(paragraph))\n",
        "\n",
        "# Tokenize the sentences\n",
        "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "idx2word = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
        "for sentence in sentences:\n",
        "    for word in sentence.split():\n",
        "        if word not in word2idx:\n",
        "            word2idx[word] = len(word2idx)\n",
        "            idx2word[len(idx2word)] = word\n",
        "\n",
        "# Convert the sentences to sequences of word indices\n",
        "sequences = []\n",
        "for sentence in sentences:\n",
        "    sequence = []\n",
        "    for word in sentence.split():\n",
        "        if word in word2idx:\n",
        "            sequence.append(word2idx[word])\n",
        "        else:\n",
        "            sequence.append(word2idx[\"<UNK>\"])\n",
        "    sequences.append(sequence)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_len = max([len(sequence) for sequence in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding=\"post\", value=word2idx[\"<PAD>\"])\n",
        "\n",
        "# Generate input and output data\n",
        "X = padded_sequences[:, :-1]\n",
        "y = padded_sequences[:, 1:]\n",
        "\n",
        "y_onehot = np.zeros((len(sequences), max_len, len(word2idx)))\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for j, word_idx in enumerate(sequence):\n",
        "        y_onehot[i, j, word_idx] = 1\n",
        "y = y_onehot[:, :-1, :]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word2idx), output_dim=50, input_length=max_len-1))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(Dense(len(word2idx), activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# Train the LSTM model\n",
        "model.fit(X, y, epochs=100)\n",
        "\n",
        "# Function to generate a question from a sentence\n",
        "def generate_question(sentence):\n",
        "    # Tokenize the sentence\n",
        "    words = sentence.lower().split()\n",
        "    # Convert the words to word indices\n",
        "    sequence = []\n",
        "    for word in words:\n",
        "        if word in word2idx:\n",
        "            sequence.append(word2idx[word])\n",
        "    # Pad the sequence\n",
        "    sequence = pad_sequences([sequence], maxlen=max_len-1)\n",
        "    # Make the prediction\n",
        "    prediction = model.predict(sequence)\n",
        "    # Convert the prediction to a word\n",
        "    predicted_word_idx = np.argmax(prediction)\n",
        "    predicted_word = idx2word.get(predicted_word_idx, \"\")\n",
        "    # Generate the question\n",
        "    if predicted_word:\n",
        "        question = f\"What is {predicted_word} in the sentence \\\"{sentence}\\\"?\"\n",
        "    else:\n",
        "        question = \"\"\n",
        "    return question\n",
        "\n",
        "# Test the function\n",
        "sentence = \"The sky is blue.\"\n",
        "question = generate_question(sentence)\n",
        "print(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHglcEV6PZk8"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import logging\n",
        "from typing import Optional, Dict, Union\n",
        "from nltk import sent_tokenize\n",
        "from langdetect import detect\n",
        "import torch\n",
        "from transformers import(\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from transformers import pipeline\n",
        "nltk.download('wordnet')\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "!pip install googletrans==3.1.0a0\n",
        "from googletrans import Translator\n",
        "ans=[]\n",
        "class QGPipeline:\n",
        "    \"\"\"Poor man's QG pipeline\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: PreTrainedModel,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        ans_model: PreTrainedModel,\n",
        "        ans_tokenizer: PreTrainedTokenizer,\n",
        "        qg_format: str,\n",
        "        use_cuda: bool\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.ans_model = ans_model\n",
        "        self.ans_tokenizer = ans_tokenizer\n",
        "\n",
        "        self.qg_format = qg_format\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        if self.ans_model is not self.model:\n",
        "            self.ans_model.to(self.device)\n",
        "\n",
        "        assert self.model.__class__.__name__ in [\"T5ForConditionalGeneration\", \"BartForConditionalGeneration\"]\n",
        "\n",
        "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
        "            self.model_type = \"t5\"\n",
        "        else:\n",
        "            self.model_type = \"bart\"\n",
        "\n",
        "    #def generate_distractors(target_word, num_distractors):\n",
        "        # Find synonyms, antonyms, hyponyms, and hypernyms of the target word\n",
        "\n",
        "\n",
        "    def __call__(self, inputs: str):\n",
        "        f=detect(inputs)\n",
        "        t = Translator()\n",
        "        x = t.translate(inputs)\n",
        "        inputs=x.text\n",
        "        inputs = \" \".join(inputs.split())\n",
        "        sents, answers = self._extract_answers(inputs)\n",
        "        flat_answers = list(itertools.chain(*answers))\n",
        "\n",
        "        if len(flat_answers) == 0:\n",
        "          return []\n",
        "\n",
        "        if self.qg_format == \"prepend\":\n",
        "            qg_examples = self._prepare_inputs_for_qg_from_answers_prepend(inputs, answers)\n",
        "        else:\n",
        "            qg_examples = self._prepare_inputs_for_qg_from_answers_hl(sents, answers)\n",
        "\n",
        "        qg_inputs = [example['source_text'] for example in qg_examples]\n",
        "        questions = self._generate_questions(qg_inputs)\n",
        "        for example, que in zip(qg_examples, questions):\n",
        "\n",
        "            #distractors = generate_distractors(example['answer'], 3)\n",
        "            if len(example['answer'])==0:\n",
        "              continue\n",
        "            words=example['answer'].split()\n",
        "            last_word = words[-1]\n",
        "            target_word=last_word\n",
        "            num_distractors=3\n",
        "            synonyms = set()\n",
        "            antonyms = set()\n",
        "            hyponyms = set()\n",
        "            hypernyms = set()\n",
        "\n",
        "            for syn in wordnet.synsets(target_word):\n",
        "                for lemma in syn.lemmas():\n",
        "                    synonyms.add(lemma.name())\n",
        "                    if lemma.antonyms():\n",
        "                        antonyms.add(lemma.antonyms()[0].name())\n",
        "                for hypo in syn.hyponyms():\n",
        "                    for lemma in hypo.lemmas():\n",
        "                        hyponyms.add(lemma.name())\n",
        "                for hyper in syn.hypernyms():\n",
        "                    for lemma in hyper.lemmas():\n",
        "                        hypernyms.add(lemma.name())\n",
        "\n",
        "            # Generate candidate distractors using the language model\n",
        "            candidates = list(synonyms.union(antonyms).union(hyponyms).union(hypernyms))\n",
        "            distractors = []\n",
        "            for candidate in candidates:\n",
        "                if candidate != target_word:\n",
        "                    try:\n",
        "                        generated_text = generator(f\"Which is more related to {target_word}? {target_word} or {candidate}\", max_length=20, num_return_sequences=1, do_sample=True)[0]['generated_text'].strip()\n",
        "                        distractors.append((candidate, generated_text))\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            # Rank distractors by relevance and choose top N\n",
        "            distractors.sort(key=lambda x: x[1])\n",
        "            xx = t.translate(example['answer'],dest=f)\n",
        "            example['answer']=xx.text\n",
        "            xy=t.translate(que,dest=f)\n",
        "            que=xy.text\n",
        "            distractors= [d[0] for d in distractors[:num_distractors]]\n",
        "            if len(distractors)==0 and f=='hi':\n",
        "              distractors.append('जिजीविषा')\n",
        "              distractors.append('प्रेमशक्त')\n",
        "              distractors.append('तमक')\n",
        "            elif len(distractors)==0:\n",
        "              distractors.append('Morrow')\n",
        "              distractors.append('Kerfuffle')\n",
        "              distractors.append('Crapulous')\n",
        "            for m in range(len(distractors)):\n",
        "              mm=t.translate(distractors[m],dest=f)\n",
        "              distractors[m]=mm.text\n",
        "            index = random.randint(0, len(distractors))\n",
        "            distractors.insert(index,example['answer'])\n",
        "            output=[{'question': que, 'distractors':distractors,'answer': example['answer']}]\n",
        "            ans.append(output)\n",
        "            #print(output)\n",
        "        for i in range(len(ans)):\n",
        "            print(ans[i])\n",
        "        ans1=ans.copy()\n",
        "        ans.clear()\n",
        "        #print(ans)\n",
        "        return ans1\n",
        "    def _generate_questions(self, inputs):\n",
        "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
        "\n",
        "        outs = self.model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device),\n",
        "            attention_mask=inputs['attention_mask'].to(self.device),\n",
        "            max_length=32,\n",
        "            num_beams=4,\n",
        "        )\n",
        "\n",
        "        questions = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
        "        return questions\n",
        "\n",
        "    def _extract_answers(self, context):\n",
        "        sents, inputs = self._prepare_inputs_for_ans_extraction(context)\n",
        "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
        "\n",
        "        outs = self.ans_model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device),\n",
        "            attention_mask=inputs['attention_mask'].to(self.device),\n",
        "            max_length=32,\n",
        "        )\n",
        "\n",
        "        dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "        answers = [item.split('<sep>') for item in dec]\n",
        "        answers = [i[:-1] for i in answers]\n",
        "\n",
        "        return sents, answers\n",
        "\n",
        "    def _tokenize(self,\n",
        "        inputs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512\n",
        "    ):\n",
        "        inputs = self.tokenizer.batch_encode_plus(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            truncation=truncation,\n",
        "            padding=\"max_length\" if padding else False,\n",
        "            pad_to_max_length=padding,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return inputs\n",
        "\n",
        "    def _prepare_inputs_for_ans_extraction(self, text):\n",
        "        sents = sent_tokenize(text)\n",
        "\n",
        "        inputs = []\n",
        "        for i in range(len(sents)):\n",
        "            source_text = \"extract answers:\"\n",
        "            for j, sent in enumerate(sents):\n",
        "                if i == j:\n",
        "                    sent = \"<hl> %s <hl>\" % sent\n",
        "                source_text = \"%s %s\" % (source_text, sent)\n",
        "                source_text = source_text.strip()\n",
        "\n",
        "            if self.model_type == \"t5\":\n",
        "                source_text = source_text + \" </s>\"\n",
        "            inputs.append(source_text)\n",
        "\n",
        "        return sents, inputs\n",
        "\n",
        "    def _prepare_inputs_for_qg_from_answers_hl(self, sents, answers):\n",
        "        inputs = []\n",
        "        #print(answers)\n",
        "        #print(sents)\n",
        "        for i, answer in enumerate(answers):\n",
        "            if len(answer) == 0:\n",
        "              continue\n",
        "            for answer_text in answer:\n",
        "                sent = sents[i]\n",
        "                sent=sent.lower()\n",
        "                sents_copy = sents[:]\n",
        "                answer_text=answer_text[5:]\n",
        "                answer_text=answer_text.lower()\n",
        "                #print(answer_text)\n",
        "                answer_text = answer_text.strip()\n",
        "\n",
        "                ans_start_idx = sent.index(answer_text)\n",
        "\n",
        "                sent = f\"{sent[:ans_start_idx]} <hl> {answer_text} <hl> {sent[ans_start_idx + len(answer_text): ]}\"\n",
        "                sents_copy[i] = sent\n",
        "\n",
        "                source_text = \" \".join(sents_copy)\n",
        "                source_text = f\"generate question: {source_text}\"\n",
        "                if self.model_type == \"t5\":\n",
        "                    source_text = source_text + \" </s>\"\n",
        "\n",
        "                inputs.append({\"answer\": answer_text, \"source_text\": source_text})\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def _prepare_inputs_for_qg_from_answers_prepend(self, context, answers):\n",
        "        flat_answers = list(itertools.chain(*answers))\n",
        "        examples = []\n",
        "        for answer in flat_answers:\n",
        "            source_text = f\"answer: {answer} context: {context}\"\n",
        "            if self.model_type == \"t5\":\n",
        "                source_text = source_text + \" </s>\"\n",
        "\n",
        "            examples.append({\"answer\": answer, \"source_text\": source_text})\n",
        "        return examples\n",
        "\n",
        "\n",
        "class MultiTaskQAQGPipeline(QGPipeline):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def __call__(self, inputs: Union[Dict, str]):\n",
        "        if type(inputs) is str:\n",
        "            # do qg\n",
        "            return super().__call__(inputs)\n",
        "        else:\n",
        "            # do qa\n",
        "            return self._extract_answer(inputs[\"question\"], inputs[\"context\"])\n",
        "\n",
        "    def _prepare_inputs_for_qa(self, question, context):\n",
        "        source_text = f\"question: {question}  context: {context}\"\n",
        "        if self.model_type == \"t5\":\n",
        "            source_text = source_text + \" </s>\"\n",
        "        return  source_text\n",
        "\n",
        "    def _extract_answer(self, question, context):\n",
        "        source_text = self._prepare_inputs_for_qa(question, context)\n",
        "        inputs = self._tokenize([source_text], padding=False)\n",
        "\n",
        "        outs = self.model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device),\n",
        "            attention_mask=inputs['attention_mask'].to(self.device),\n",
        "            max_length=16,\n",
        "        )\n",
        "\n",
        "        answer = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
        "        return answer\n",
        "\n",
        "\n",
        "class E2EQGPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: PreTrainedModel,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        use_cuda: bool\n",
        "    ) :\n",
        "\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        assert self.model.__class__.__name__ in [\"T5ForConditionalGeneration\", \"BartForConditionalGeneration\"]\n",
        "\n",
        "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
        "            self.model_type = \"t5\"\n",
        "        else:\n",
        "            self.model_type = \"bart\"\n",
        "\n",
        "        self.default_generate_kwargs = {\n",
        "            \"max_length\": 256,\n",
        "            \"num_beams\": 4,\n",
        "            \"length_penalty\": 1.5,\n",
        "            \"no_repeat_ngram_size\": 3,\n",
        "            \"early_stopping\": True,\n",
        "        }\n",
        "\n",
        "    def __call__(self, context: str, **generate_kwargs):\n",
        "        inputs = self._prepare_inputs_for_e2e_qg(context)\n",
        "\n",
        "        # TODO: when overrding default_generate_kwargs all other arguments need to be passsed\n",
        "        # find a better way to do this\n",
        "        if not generate_kwargs:\n",
        "            generate_kwargs = self.default_generate_kwargs\n",
        "\n",
        "        input_length = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "        # max_length = generate_kwargs.get(\"max_length\", 256)\n",
        "        # if input_length < max_length:\n",
        "        #     logger.warning(\n",
        "        #         \"Your max_length is set to {}, but you input_length is only {}. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\".format(\n",
        "        #             max_length, input_length\n",
        "        #         )\n",
        "        #     )\n",
        "\n",
        "        outs = self.model.generate(\n",
        "            input_ids=inputs['input_ids'].to(self.device),\n",
        "            attention_mask=inputs['attention_mask'].to(self.device),\n",
        "            **generate_kwargs\n",
        "        )\n",
        "\n",
        "        prediction = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
        "        questions = prediction.split(\"<sep>\")\n",
        "        questions = [question.strip() for question in questions[:-1]]\n",
        "        return questions\n",
        "\n",
        "    def _prepare_inputs_for_e2e_qg(self, context):\n",
        "        source_text = f\"generate questions: {context}\"\n",
        "        if self.model_type == \"t5\":\n",
        "            source_text = source_text + \" </s>\"\n",
        "\n",
        "        inputs = self._tokenize([source_text], padding=False)\n",
        "        return inputs\n",
        "\n",
        "    def _tokenize(\n",
        "        self,\n",
        "        inputs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512\n",
        "    ):\n",
        "        inputs = self.tokenizer.batch_encode_plus(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            add_special_tokens=add_special_tokens,\n",
        "            truncation=truncation,\n",
        "            padding=\"max_length\" if padding else False,\n",
        "            pad_to_max_length=padding,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return inputs\n",
        "\n",
        "\n",
        "SUPPORTED_TASKS = {\n",
        "    \"question-generation\": {\n",
        "        \"impl\": QGPipeline,\n",
        "        \"default\": {\n",
        "            \"model\": \"valhalla/t5-small-qg-hl\",\n",
        "            \"ans_model\": \"valhalla/t5-base-qa-qg-hl\",\n",
        "        }\n",
        "    },\n",
        "    \"multitask-qa-qg\": {\n",
        "        \"impl\": MultiTaskQAQGPipeline,\n",
        "        \"default\": {\n",
        "            \"model\": \"valhalla/t5-base-qa-qg-hl\",\n",
        "        }\n",
        "    },\n",
        "    \"e2e-qg\": {\n",
        "        \"impl\": E2EQGPipeline,\n",
        "        \"default\": {\n",
        "            \"model\": \"valhalla/t5-small-e2e-qg\",\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "def pipeline(\n",
        "    task: str,\n",
        "    model = None,\n",
        "    tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
        "    qg_format: Optional[str] = \"highlight\",\n",
        "    ans_model = None,\n",
        "    ans_tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
        "    use_cuda: Optional[bool] = True,\n",
        "    **kwargs,\n",
        "):\n",
        "    # Retrieve the task\n",
        "    if task not in SUPPORTED_TASKS:\n",
        "        raise KeyError(\"Unknown task {}, available tasks are {}\".format(task, list(SUPPORTED_TASKS.keys())))\n",
        "\n",
        "    targeted_task = SUPPORTED_TASKS[task]\n",
        "    task_class = targeted_task[\"impl\"]\n",
        "\n",
        "    # Use default model/config/tokenizer for the task if no model is provided\n",
        "    if model is None:\n",
        "        model = targeted_task[\"default\"][\"model\"]\n",
        "\n",
        "    # Try to infer tokenizer from model or config name (if provided as str)\n",
        "    if tokenizer is None:\n",
        "        if isinstance(model, str):\n",
        "            tokenizer = model\n",
        "        else:\n",
        "            # Impossible to guest what is the right tokenizer here\n",
        "            raise Exception(\n",
        "                \"Impossible to guess which tokenizer to use. \"\n",
        "                \"Please provided a PretrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
        "            )\n",
        "\n",
        "    # Instantiate tokenizer if needed\n",
        "    if isinstance(tokenizer, (str, tuple)):\n",
        "        if isinstance(tokenizer, tuple):\n",
        "            # For tuple we have (tokenizer name, {kwargs})\n",
        "            tokenizer = AutoTokenizer.from_pretrained(tokenizer[0], **tokenizer[1])\n",
        "        else:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
        "\n",
        "    # Instantiate model if needed\n",
        "    if isinstance(model, str):\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
        "\n",
        "    if task == \"question-generation\":\n",
        "        if ans_model is None:\n",
        "            # load default ans model\n",
        "            ans_model = targeted_task[\"default\"][\"ans_model\"]\n",
        "            ans_tokenizer = AutoTokenizer.from_pretrained(ans_model)\n",
        "            ans_model = AutoModelForSeq2SeqLM.from_pretrained(ans_model)\n",
        "        else:\n",
        "            # Try to infer tokenizer from model or config name (if provided as str)\n",
        "            if ans_tokenizer is None:\n",
        "                if isinstance(ans_model, str):\n",
        "                    ans_tokenizer = ans_model\n",
        "                else:\n",
        "                    # Impossible to guest what is the right tokenizer here\n",
        "                    raise Exception(\n",
        "                        \"Impossible to guess which tokenizer to use. \"\n",
        "                        \"Please provided a PretrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
        "                    )\n",
        "\n",
        "            # Instantiate tokenizer if needed\n",
        "            if isinstance(ans_tokenizer, (str, tuple)):\n",
        "                if isinstance(ans_tokenizer, tuple):\n",
        "                    # For tuple we have (tokenizer name, {kwargs})\n",
        "                    ans_tokenizer = AutoTokenizer.from_pretrained(ans_tokenizer[0], **ans_tokenizer[1])\n",
        "                else:\n",
        "                    ans_tokenizer = AutoTokenizer.from_pretrained(ans_tokenizer)\n",
        "\n",
        "            if isinstance(ans_model, str):\n",
        "                ans_model = AutoModelForSeq2SeqLM.from_pretrained(ans_model)\n",
        "\n",
        "    if task == \"e2e-qg\":\n",
        "        return task_class(model=model, tokenizer=tokenizer, use_cuda=use_cuda)\n",
        "    elif task == \"question-generation\":\n",
        "        return task_class(model=model, tokenizer=tokenizer, ans_model=ans_model, ans_tokenizer=ans_tokenizer, qg_format=qg_format, use_cuda=use_cuda)\n",
        "    else:\n",
        "        return task_class(model=model, tokenizer=tokenizer, ans_model=model, ans_tokenizer=tokenizer, qg_format=qg_format, use_cuda=use_cuda)\n",
        "print(ans)\n",
        "print(len(ans))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFSZiIc0StHY"
      },
      "outputs": [],
      "source": [
        "nlp = pipeline(\"question-generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbDVLXDpP0YQ"
      },
      "outputs": [],
      "source": [
        "answers = [['<pad> Python'], ['<pad> Guido van Rossum']]\n",
        "sents = ['Python is an interpreted, high-level, general-purpose programming language.', \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "furVTILRP3SN"
      },
      "outputs": [],
      "source": [
        "inputs = []\n",
        "# print(answers)\n",
        "# print(sents)\n",
        "for i, answer in enumerate(answers):\n",
        "  print(answer)\n",
        "  if len(answer) == 0: continue\n",
        "  for answer_text in answer:\n",
        "      print(answer_text)\n",
        "      sent = sents[i]\n",
        "      sents_copy = sents[:]\n",
        "\n",
        "      answer_text = answer_text.split(\" \")[1]\n",
        "      print(answer_text)\n",
        "      print(sent)\n",
        "      ans_start_idx = sent.index(answer_text)\n",
        "\n",
        "      sent = f\"{sent[:ans_start_idx]} <hl> {answer_text} <hl> {sent[ans_start_idx + len(answer_text): ]}\"\n",
        "      sents_copy[i] = sent\n",
        "\n",
        "      source_text = \" \".join(sents_copy)\n",
        "      source_text = f\"generate question: {source_text}\"\n",
        "      # if self.model_type == \"t5\":\n",
        "      #     source_text = source_text + \" </s>\"\n",
        "\n",
        "      inputs.append({\"answer\": answer_text, \"source_text\": source_text})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DHB0dDqTb-o"
      },
      "source": [
        "If you want to use the t5-base model, then pass the path through model parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_050CddNTWeU"
      },
      "outputs": [],
      "source": [
        "nlp = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYGczss6EJHM"
      },
      "outputs": [],
      "source": [
        "'''from flask import Flask, render_template, request\n",
        "from transformers import pipeline\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Initialize the question generation pipeline with the specified model\n",
        "nlp = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Function to generate question\n",
        "def generate_question(sentence, answer):\n",
        "    ans_start_idx = sentence.lower().find(answer.lower())\n",
        "    if ans_start_idx == -1:\n",
        "        return \"Answer not found in the sentence.\"\n",
        "\n",
        "    sent = f\"{sentence[:ans_start_idx]} <hl> {answer} <hl> {sentence[ans_start_idx + len(answer):]}\"\n",
        "    source_text = f\"generate question: {sent}\"\n",
        "    question = nlp(source_text)\n",
        "\n",
        "    return question[0]['generated_text']\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
        "def index():\n",
        "    if request.method == \"POST\":\n",
        "        sentence = request.form[\"sentence\"]\n",
        "        answer = request.form[\"answer\"]\n",
        "        question = generate_question(sentence, answer)\n",
        "        return render_template(\"index.html\", question=question)\n",
        "    return render_template(\"index.html\", question=None)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Open a tunnel to the Flask app on port 5000\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"Your ngrok URL is: {public_url}\")\n",
        "\n",
        "    # Run Flask app on 0.0.0.0 to make it publicly accessible\n",
        "    app.run(host=\"0.0.0.0\", port=5000)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oth-qvaoLTc-"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok\n",
        "\n",
        "# Paste your ngrok auth token here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU5y_LBWZRSk"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2pKU3TumDND3Y2NiHmxpDpQI9od_6qaNnyqsSzh2LmFUCv93f\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL3W1YohNNAw"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import logging\n",
        "\n",
        "# Configure ngrok to log errors\n",
        "conf.get_default().log_event_callback = lambda log: print(log)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBGKmodyJvKo"
      },
      "outputs": [],
      "source": [
        "# Doing the fuck work\n",
        "!pip install flask pyngrok transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPK0OImC1a6a"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sobMOP2rJ34Z"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, render_template_string, request\n",
        "from werkzeug.utils import secure_filename\n",
        "from transformers import pipeline\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import PyPDF2\n",
        "import random\n",
        "\n",
        "# Initialize the pipelines\n",
        "nlp_qg = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
        "nlp_ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "app.config[\"UPLOAD_FOLDER\"] = \"./uploads\"\n",
        "os.makedirs(app.config[\"UPLOAD_FOLDER\"], exist_ok=True)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to detect answers using NER\n",
        "def detect_answers(sentence):\n",
        "    entities = nlp_ner(sentence)\n",
        "    return [entity['word'] for entity in entities]\n",
        "\n",
        "# Function to generate MCQs\n",
        "def generate_mcqs(text):\n",
        "    sentences = text.split(\". \")  # Split text into sentences\n",
        "    mcqs = []\n",
        "    for sentence in sentences:\n",
        "        answers = detect_answers(sentence)\n",
        "        for answer in answers:\n",
        "            ans_start_idx = sentence.lower().find(answer.lower())\n",
        "            sent = f\"{sentence[:ans_start_idx]} <hl> {answer} <hl> {sentence[ans_start_idx + len(answer):]}\"\n",
        "            source_text = f\"generate question: {sent}\"\n",
        "            question = nlp_qg(source_text)[0]['generated_text']\n",
        "\n",
        "            # Generate multiple-choice options\n",
        "            distractors = [ans for ans in answers if ans != answer]\n",
        "            distractors = list(set(distractors))  # Ensure unique distractors\n",
        "            random.shuffle(distractors)\n",
        "            options = [answer] + distractors[:3]\n",
        "            random.shuffle(options)  # Shuffle options\n",
        "\n",
        "            mcqs.append({\n",
        "                \"question\": question,\n",
        "                \"options\": options,\n",
        "                \"answer\": answer\n",
        "            })\n",
        "    return mcqs\n",
        "\n",
        "# Function to generate only questions\n",
        "def generate_questions(text):\n",
        "    sentences = text.split(\". \")  # Split text into sentences\n",
        "    questions = []\n",
        "    for sentence in sentences:\n",
        "        answers = detect_answers(sentence)\n",
        "        for answer in answers:\n",
        "            ans_start_idx = sentence.lower().find(answer.lower())\n",
        "            sent = f\"{sentence[:ans_start_idx]} <hl> {answer} <hl> {sentence[ans_start_idx + len(answer):]}\"\n",
        "            source_text = f\"generate question: {sent}\"\n",
        "            question = nlp_qg(source_text)[0]['generated_text']\n",
        "            questions.append(question)\n",
        "    return questions\n",
        "\n",
        "# Flask route to handle text and PDF upload\n",
        "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
        "def index():\n",
        "    output = None\n",
        "    option = None\n",
        "    if request.method == \"POST\":\n",
        "        option = request.form.get(\"generation_option\", \"questions\")\n",
        "        input_text = request.form.get(\"input_text\", \"\").strip()\n",
        "        text = input_text\n",
        "\n",
        "        # Handle PDF file upload\n",
        "        if \"pdf_file\" in request.files:\n",
        "            pdf_file = request.files[\"pdf_file\"]\n",
        "            if pdf_file.filename != \"\":\n",
        "                file_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], secure_filename(pdf_file.filename))\n",
        "                pdf_file.save(file_path)\n",
        "                text = extract_text_from_pdf(file_path)\n",
        "\n",
        "        # Generate based on selected option\n",
        "        if text:\n",
        "            if option == \"mcqs\":\n",
        "                output = generate_mcqs(text)\n",
        "            else:\n",
        "                output = generate_questions(text)\n",
        "\n",
        "    # HTML Template\n",
        "    html_template = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"en\">\n",
        "    <head>\n",
        "        <title>Interactive Question Generator</title>\n",
        "        <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
        "    </head>\n",
        "    <body class=\"bg-light\">\n",
        "        <div class=\"container mt-5\">\n",
        "            <div class=\"card shadow\">\n",
        "                <div class=\"card-body\">\n",
        "                    <h2 class=\"text-center text-primary mb-4\">Interactive Question Generator</h2>\n",
        "                    <form method=\"POST\" enctype=\"multipart/form-data\">\n",
        "                        <div class=\"mb-3\">\n",
        "                            <label for=\"generation_option\" class=\"form-label\">Select an Option:</label>\n",
        "                            <div>\n",
        "                                <input type=\"radio\" id=\"mcqs\" name=\"generation_option\" value=\"mcqs\" {% if option == 'mcqs' %}checked{% endif %}>\n",
        "                                <label for=\"mcqs\">Generate MCQs</label>\n",
        "                                <input type=\"radio\" id=\"questions\" name=\"generation_option\" value=\"questions\" class=\"ms-3\" {% if option == 'questions' %}checked{% endif %}>\n",
        "                                <label for=\"questions\">Only Generate Questions</label>\n",
        "                            </div>\n",
        "                        </div>\n",
        "                        <div class=\"mb-3\">\n",
        "                            <label for=\"input_text\" class=\"form-label\">Enter Text:</label>\n",
        "                            <textarea id=\"input_text\" name=\"input_text\" class=\"form-control\" rows=\"4\" placeholder=\"Type your text here...\"></textarea>\n",
        "                        </div>\n",
        "                        <div class=\"mb-3\">\n",
        "                            <label for=\"pdf_file\" class=\"form-label\">Or Upload a PDF:</label>\n",
        "                            <input type=\"file\" id=\"pdf_file\" name=\"pdf_file\" class=\"form-control\" accept=\".pdf\">\n",
        "                        </div>\n",
        "                        <button type=\"submit\" class=\"btn btn-primary w-100\">Generate</button>\n",
        "                    </form>\n",
        "                </div>\n",
        "            </div>\n",
        "            {% if output %}\n",
        "                <div class=\"card mt-4 shadow\">\n",
        "                    <div class=\"card-body\">\n",
        "                        {% if option == 'mcqs' %}\n",
        "                            <h3 class=\"text-success\">Generated MCQs:</h3>\n",
        "                            <ul class=\"list-group\">\n",
        "                                {% for mcq in output %}\n",
        "                                    <li class=\"list-group-item\">\n",
        "                                        <strong>{{ mcq.question }}</strong>\n",
        "                                        <ol>\n",
        "                                        {% for option in mcq.options %}\n",
        "                                            <li>{{ option }}</li>\n",
        "                                        {% endfor %}\n",
        "                                        </ol>\n",
        "                                    </li>\n",
        "                                {% endfor %}\n",
        "                            </ul>\n",
        "                        {% else %}\n",
        "                            <h3 class=\"text-success\">Generated Questions:</h3>\n",
        "                            <ul class=\"list-group\">\n",
        "                                {% for question in output %}\n",
        "                                    <li class=\"list-group-item\">{{ question }}</li>\n",
        "                                {% endfor %}\n",
        "                            </ul>\n",
        "                        {% endif %}\n",
        "                    </div>\n",
        "                </div>\n",
        "            {% endif %}\n",
        "        </div>\n",
        "        <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js\"></script>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return render_template_string(html_template, output=output, option=option)\n",
        "\n",
        "# Start Flask server and ngrok tunnel\n",
        "public_url = ngrok.connect(5001)\n",
        "print(\"Access the app at:\", public_url)\n",
        "\n",
        "app.run(port=5001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1xJtE4oUOQPj",
        "outputId": "c85a561d-0aac-4e36-80a0-f27b6ad9b5d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ngrok - tunnel local ports to public URLs and inspect traffic\n",
            "\n",
            "USAGE:\n",
            "  ngrok [command] [flags]\n",
            "\n",
            "AUTHOR:\n",
            "  ngrok - <support@ngrok.com>\n",
            "\n",
            "COMMANDS: \n",
            "  config          update or migrate ngrok's configuration file\n",
            "  http            start an HTTP tunnel\n",
            "  tcp             start a TCP tunnel\n",
            "  tunnel          start a tunnel for use with a tunnel-group backend\n",
            "\n",
            "EXAMPLES: \n",
            "  ngrok http 80                                                 # secure public URL for port 80 web server\n",
            "  ngrok http --url baz.ngrok.dev 8080                           # port 8080 available at baz.ngrok.dev\n",
            "  ngrok tcp 22                                                  # tunnel arbitrary TCP traffic to port 22\n",
            "  ngrok http 80 --oauth=google --oauth-allow-email=foo@foo.com  # secure your app with oauth\n",
            "\n",
            "Paid Features: \n",
            "  ngrok http 80 --url mydomain.com                              # run ngrok with your own custom domain\n",
            "  ngrok http 80 --cidr-allow 2600:8c00::a03c:91ee:fe69:9695/32  # run ngrok with IP policy restrictions\n",
            "  Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Flags:\n",
            "  -h, --help      help for ngrok\n",
            "\n",
            "Use \"ngrok [command] --help\" for more information about a command.\n"
          ]
        }
      ],
      "source": [
        "!ngrok tunnels list\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}